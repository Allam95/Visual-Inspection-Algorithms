{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "\n",
    "import funcy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import detectron2\n",
    "import detectron2.data.datasets\n",
    "import detectron2.data.catalog\n",
    "import detectron2.utils.logger\n",
    "import detectron2.utils.visualizer\n",
    "import detectron2.config\n",
    "import detectron2.engine\n",
    "from detectron2.engine import DefaultPredictor\n",
    "import detectron2.evaluation\n",
    "import detectron2.model_zoo\n",
    "\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import detectron2.evaluation\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_coco(file, info, licenses, images, annotations, categories):\n",
    "    with open(file, 'wt', encoding='UTF-8') as coco:\n",
    "        json.dump({ 'info': info, 'licenses': licenses, 'images': images, \n",
    "            'annotations': annotations, 'categories': categories}, coco, indent=2, sort_keys=True)\n",
    "\n",
    "def filter_annotations(annotations, images):\n",
    "    image_ids = funcy.lmap(lambda i: int(i['id']), images)\n",
    "    return funcy.lfilter(lambda a: int(a['image_id']) in image_ids, annotations)\n",
    "\n",
    "def train_validate_test_split(path_to_coco_json_file, train_size=0.8, validate_size=0.2): # 0.8 train, 0.2 validate, testset is uploaded from other file\n",
    "    with open(path_to_coco_json_file, 'rt', encoding='UTF-8') as annotations:\n",
    "        coco = json.load(annotations)\n",
    "        info = coco.get('info')\n",
    "        licenses = coco.get('licenses')\n",
    "        images = coco.get('images')\n",
    "        annotations = coco.get('annotations')\n",
    "        categories = coco.get('categories')\n",
    "\n",
    "        number_of_images = len(images)\n",
    "\n",
    "        images_with_annotations = funcy.lmap(lambda a: int(a['image_id']), annotations)\n",
    "\n",
    "        images = funcy.lremove(lambda i: i['id'] not in images_with_annotations, images)\n",
    "        random.shuffle(images)\n",
    "\n",
    "        train, validate, test = np.split(images, [int(train_size*len(images)), int((train_size + validate_size)*len(images))])\n",
    "\n",
    "        base_path = os.path.splitext(path_to_coco_json_file)[0]\n",
    "        train_path = base_path + '-train.json'\n",
    "        validate_path = base_path + '-validate.json'\n",
    "        test_path = base_path + '-test.json'\n",
    "        \n",
    "        save_coco(train_path, info, licenses, list(train), filter_annotations(annotations, train), categories)\n",
    "        save_coco(validate_path, info, licenses, list(validate), filter_annotations(annotations, validate), categories)\n",
    "        save_coco(test_path, info, licenses, list(test), filter_annotations(annotations, test), categories)\n",
    "\n",
    "        print(\"Saved {} entries in {} \\nSaved {} entries in {} \\nSaved {} entries in {}\".format(len(train), train_path, len(validate), validate_path, len(test), test_path))\n",
    "        \n",
    "        return train_path, validate_path, test_path,coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validate sets only, test set is loaded from other file\n",
    "train_file_path, validate_file_path, test_file_path,coco_return = train_validate_test_split('Custom-testest-labeled-data/Damaged_teeth_trainval-24.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"Chipped-Tooth-Images\"\n",
    "detectron2.data.datasets.register_coco_instances(\"train\", {}, train_file_path, image_path)\n",
    "detectron2.data.datasets.register_coco_instances(\"validate\", {}, validate_file_path, image_path)\n",
    "detectron2.data.datasets.register_coco_instances(\"test\", {}, 'Custom-testest-labeled-data/Damaged_teeth_testset_30_gears-23.json', image_path) # load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_train = detectron2.data.catalog.DatasetCatalog.get(\"train\")\n",
    "metadata_train = detectron2.data.MetadataCatalog.get(\"train\")\n",
    "\n",
    "catalog_validate = detectron2.data.catalog.DatasetCatalog.get(\"validate\")\n",
    "metadata_validate = detectron2.data.MetadataCatalog.get(\"validate\")\n",
    "\n",
    "catalog_test = detectron2.data.catalog.DatasetCatalog.get(\"test\")\n",
    "metadata_test = detectron2.data.MetadataCatalog.get(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(catalog_train,1)[0]\n",
    "img = cv2.imread(sample[\"file_name\"])[:, :, ::-1] \n",
    "visualizer = detectron2.utils.visualizer.Visualizer(img, metadata_train, scale=1)\n",
    "vis = visualizer.draw_dataset_dict(sample)\n",
    "plt.figure()\n",
    "plt.title(sample[\"file_name\"])\n",
    "plt.imshow(vis.get_image())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = detectron2.config.get_cfg()\n",
    "cfg.merge_from_file(detectron2.model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = (\"validate\",)\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4 \n",
    "cfg.SOLVER.BASE_LR = 0.01\n",
    "cfg.SOLVER.GAMMA = 0.05 \n",
    "cfg.SOLVER.MAX_ITER = 10000 \n",
    "cfg.TEST.EVAL_PERIOD = cfg.SOLVER.MAX_ITER // 10 \n",
    "cfg.OUTPUT_DIR = \"./output\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTrainer(detectron2.engine.DefaultTrainer):\n",
    "    \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        return detectron2.evaluation.COCOEvaluator(dataset_name, cfg, False, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CocoTrainer(cfg)\n",
    "trainer.resume_or_load(resume= True) # True if you want to continue from last checkpoint\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DI \"model_final.pth\" # use it to evaluate the recent trained model\n",
    "#cfg.MODEL.WEIGHTS = \"Weights/New_Chipped_Tooth_Weights.pth\"  # use it to loads the previously trained weights\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0 # non-maximum suppression \n",
    "cfg.DATASETS.TEST = (\"test\", )\n",
    "predictor = detectron2.engine.DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect three random images from the test set\n",
    "samples = random.sample(catalog_test, 3) \n",
    "print(len(catalog_test))\n",
    "for sample in samples:\n",
    "    img = cv2.imread(sample[\"file_name\"])\n",
    "    prediction = predictor(img)\n",
    "    visualizer_ground_truth = detectron2.utils.visualizer.Visualizer(img[:, :, ::-1], metadata_test, scale=1)  \n",
    "    visualizer_predictions = detectron2.utils.visualizer.Visualizer(img[:, :, ::-1], metadata_test, scale=1)\n",
    "    #show ground truth\n",
    "    vis_ground_truth = visualizer_ground_truth.draw_dataset_dict(sample)\n",
    "    #show predictions\n",
    "    vis_predictions = visualizer_predictions.draw_instance_predictions(prediction[\"instances\"].to(\"cpu\"))   \n",
    "    plt.figure()\n",
    "    plt.title(sample[\"file_name\"])   \n",
    "    plt.imshow(vis_ground_truth.get_image()) \n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.title(sample[\"file_name\"])\n",
    "    plt.imshow(vis_predictions.get_image())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a specific image of the test set\n",
    "inspected_image = cv2.imread('path-to-image')\n",
    "predicted_instances = predictor(inspected_image)\n",
    "visualizer_predictions_inspected = detectron2.utils.visualizer.Visualizer(inspected_image[:, :, ::-1], metadata_test, scale=1)\n",
    "vis_predictions_inspected = visualizer_predictions_inspected.draw_instance_predictions(predicted_instances[\"instances\"].to(\"cpu\"))\n",
    "print(predicted_instances[\"instances\"].get('pred_boxes'))\n",
    "plt.imshow(vis_predictions_inspected.get_image())\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run these cells to plot the precision and recall values at specific thresholds (.75 to .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics(threshold_for_json):\n",
    "    \"\"\"Exports two json file which contain the ground truth and predictions bboxes and image's name in a certain format\n",
    "\n",
    "    Args:\n",
    "        Different thresholds\n",
    "\n",
    "    Returns:\n",
    "        saves two json files, one for ground truth boxes and the other for prediction boxes\n",
    "\n",
    "    \"\"\"\n",
    "    predictions_dict ={}\n",
    "    ground_truth_dict = {}\n",
    "    false_positives_images = []\n",
    "    false_negatives_images = []\n",
    "    total_ground_truth = 0\n",
    "    true_defects_number = 0\n",
    "    for example in catalog_test :\n",
    "        annotations_array=[]      \n",
    "        img = cv2.imread(example[\"file_name\"])\n",
    "        prediction = predictor(img)\n",
    "        \n",
    "        \n",
    "        visualizer_ground_truth = detectron2.utils.visualizer.Visualizer(img[:, :, ::-1], metadata_test, scale=1)\n",
    "        visualizer_predictions = detectron2.utils.visualizer.Visualizer(img[:, :, ::-1], metadata_test, scale=1)\n",
    "        vis_ground_truth = visualizer_ground_truth.draw_dataset_dict(example)        \n",
    "        vis_predictions = visualizer_predictions.draw_instance_predictions(prediction[\"instances\"].to(\"cpu\"))\n",
    "        \n",
    "        # uncomment the following lines to show ground truth\n",
    "        \n",
    "#         plt.figure() \n",
    "#         plt.title(example[\"file_name\"]) \n",
    "#         plt.imshow(vis_ground_truth.get_image()) \n",
    "#         plt.show() \n",
    "        \n",
    "        # uncomment the following lines to show predictions\n",
    "        \n",
    "#         plt.figure() \n",
    "#         plt.title(example[\"file_name\"]) \n",
    "#         plt.imshow(vis_predictions.get_image()) \n",
    "#         plt.show() \n",
    "        \n",
    "        # Getting bboxes for ground truth\n",
    "        total_ground_truth = total_ground_truth + len(example['annotations'])\n",
    "        for anno in range (len(example['annotations'])):\n",
    "            bounding_coord = example['annotations'][anno]['bbox']\n",
    "\n",
    "            modified_boxes = [bounding_coord[0],bounding_coord[1],bounding_coord[0]+bounding_coord[2],bounding_coord[1]+bounding_coord[3]]\n",
    "\n",
    "            annotations_array.append(modified_boxes)\n",
    "       \n",
    "        ground_dict= {\n",
    "          os.path.basename(example[\"file_name\"]):annotations_array\n",
    "        }\n",
    "        ground_truth_dict.update(ground_dict) \n",
    "   \n",
    "        with open(\"ground_truth_chipped_all\"+str(threshold_for_json)+\".json\", \"w\") as outfile: \n",
    "            json.dump(ground_truth_dict, outfile)\n",
    "\n",
    "\n",
    "        predictions_boxes = prediction[\"instances\"].get('pred_boxes')\n",
    "        predictions_scores = prediction[\"instances\"].get('scores')\n",
    "        boxes_array=[]\n",
    "        scores_array=[]\n",
    "\n",
    "        # Getting bboxes and scores for predicitons\n",
    "\n",
    "        for predict in range(len(predictions_boxes)):\n",
    "            coordinates = [round(float(list(predictions_boxes)[predict][0]),4),round(float(list(predictions_boxes)[predict][1]),4),round(float(list(predictions_boxes)[predict][2]),4),round(float(list(predictions_boxes)[predict][3]),4)]\n",
    "            scores = round(float(predictions_scores[predict]),4)\n",
    "            boxes_array.append(coordinates)\n",
    "            scores_array.append(scores)\n",
    "        pred_dict= {\n",
    "          os.path.basename(example[\"file_name\"]):{\n",
    "          \"boxes\": boxes_array,\n",
    "          \"scores\": scores_array\n",
    "        }}\n",
    "        predictions_dict.update(pred_dict)        \n",
    "        with open(\"predictions_chipped_all\"+str(threshold_for_json)+\".json\", \"w\") as outfile: \n",
    "            json.dump(predictions_dict, outfile) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "author: Timothy C. Arlen\n",
    "date: 28 Feb 2018\n",
    "\n",
    "edited by : Abdelrahman Allam\n",
    "\"\"\"\n",
    "\n",
    "COLORS = [\n",
    "    '#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n",
    "    '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n",
    "    '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n",
    "    '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5']\n",
    "\n",
    "\n",
    "def calc_iou_individual(pred_box, gt_box):\n",
    "    \"\"\"Calculate IoU of single predicted and ground truth box\n",
    "\n",
    "    Args:\n",
    "        pred_box (list of floats): location of predicted object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "        gt_box (list of floats): location of ground truth object as\n",
    "            [xmin, ymin, xmax, ymax]\n",
    "\n",
    "    Returns:\n",
    "        float: value of the IoU for the two boxes.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: if the box is obviously malformed\n",
    "    \"\"\"\n",
    "    x1_t, y1_t, x2_t, y2_t = gt_box\n",
    "    x1_p, y1_p, x2_p, y2_p = pred_box\n",
    "\n",
    "    if (x1_p > x2_p) or (y1_p > y2_p):\n",
    "        raise AssertionError(\n",
    "            \"Prediction box is malformed? pred box: {}\".format(pred_box))\n",
    "    if (x1_t > x2_t) or (y1_t > y2_t):\n",
    "        raise AssertionError(\n",
    "            \"Ground Truth box is malformed? true box: {}\".format(gt_box))\n",
    "\n",
    "    if (x2_t < x1_p or x2_p < x1_t or y2_t < y1_p or y2_p < y1_t):\n",
    "        return 0.0\n",
    "\n",
    "    far_x = np.min([x2_t, x2_p])\n",
    "    near_x = np.max([x1_t, x1_p])\n",
    "    far_y = np.min([y2_t, y2_p])\n",
    "    near_y = np.max([y1_t, y1_p])\n",
    "\n",
    "    inter_area = (far_x - near_x + 1) * (far_y - near_y + 1)\n",
    "    true_box_area = (x2_t - x1_t + 1) * (y2_t - y1_t + 1)\n",
    "    pred_box_area = (x2_p - x1_p + 1) * (y2_p - y1_p + 1)\n",
    "    iou = inter_area / (true_box_area + pred_box_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "\n",
    "    all_pred_indices = range(len(pred_boxes))\n",
    "    all_gt_indices = range(len(gt_boxes))\n",
    "    if len(all_pred_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = len(gt_boxes)\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "    if len(all_gt_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = len(pred_boxes)\n",
    "        fn = 0\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "\n",
    "    gt_idx_thr = []\n",
    "    pred_idx_thr = []\n",
    "    ious = []\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou = calc_iou_individual(pred_box, gt_box)\n",
    "            if iou > iou_thr:\n",
    "                gt_idx_thr.append(igb)\n",
    "                pred_idx_thr.append(ipb)\n",
    "                ious.append(iou)\n",
    "\n",
    "    args_desc = np.argsort(ious)[::-1]\n",
    "    if len(args_desc) == 0:\n",
    "        # No matches\n",
    "        tp = 0\n",
    "        fp = len(pred_boxes)\n",
    "        fn = len(gt_boxes)\n",
    "    else:\n",
    "        gt_match_idx = []\n",
    "        pred_match_idx = []\n",
    "        for idx in args_desc:\n",
    "            gt_idx = gt_idx_thr[idx]\n",
    "            pr_idx = pred_idx_thr[idx]\n",
    "            # If the boxes are unmatched, add them to matches\n",
    "            if (gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pr_idx)\n",
    "        tp = len(gt_match_idx)\n",
    "        fp = len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "\n",
    "    return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "\n",
    "\n",
    "def calc_precision_recall(img_results):\n",
    "    \"\"\"Calculates precision and recall from the set of images\n",
    "\n",
    "    Args:\n",
    "        img_results (dict): dictionary formatted like:\n",
    "            {\n",
    "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
    "                'img_id2': ...\n",
    "                ...\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        tuple: of floats of (precision, recall)\n",
    "    \"\"\"\n",
    "    true_pos = 0; false_pos = 0; false_neg = 0   \n",
    "    for _, res in img_results.items():\n",
    "        true_pos += res['true_pos']\n",
    "        false_pos += res['false_pos']\n",
    "        false_neg += res['false_neg']\n",
    "    try:\n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "    try:\n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "\n",
    "    return (precision, recall,true_pos,false_pos,false_neg)\n",
    "\n",
    "def get_model_scores_map(pred_boxes):\n",
    "    \"\"\"Creates a dictionary of from model_scores to image ids.\n",
    "\n",
    "    Args:\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' and 'scores'\n",
    "\n",
    "    Returns:\n",
    "        dict: keys are model_scores and values are image ids (usually filenames)\n",
    "\n",
    "    \"\"\"\n",
    "    model_scores_map = {}\n",
    "    for img_id, val in pred_boxes.items():\n",
    "        for score in val['scores']:\n",
    "            if score not in model_scores_map.keys():\n",
    "                model_scores_map[score] = [img_id]\n",
    "            else:\n",
    "                model_scores_map[score].append(img_id)\n",
    "    return model_scores_map\n",
    "\n",
    "def get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=0.5):\n",
    "    \"\"\"Calculates average precision at given IoU threshold.\n",
    "\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (list of list of floats): list of locations of predicted\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "\n",
    "    Returns:\n",
    "        dict: avg precision as well as summary info about the PR curve\n",
    "\n",
    "        Keys:\n",
    "            'avg_prec' (float): average precision for this IoU threshold\n",
    "            'precisions' (list of floats): precision value for the given\n",
    "                model_threshold\n",
    "            'recall' (list of floats): recall value for given\n",
    "                model_threshold\n",
    "            'models_thrs' (list of floats): model threshold value that\n",
    "                precision and recall were computed for.\n",
    "    \"\"\"\n",
    "    model_scores_map = get_model_scores_map(pred_boxes)\n",
    "    sorted_model_scores = sorted(model_scores_map.keys())\n",
    "    # Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
    "    for img_id in pred_boxes.keys():\n",
    "        arg_sort = np.argsort(pred_boxes[img_id]['scores'])\n",
    "        pred_boxes[img_id]['scores'] = np.array(pred_boxes[img_id]['scores'])[arg_sort].tolist()\n",
    "        pred_boxes[img_id]['boxes'] = np.array(pred_boxes[img_id]['boxes'])[arg_sort].tolist()\n",
    "\n",
    "    pred_boxes_pruned = deepcopy(pred_boxes)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    model_thrs = []\n",
    "    img_results = {}\n",
    "    false_positives_images_names = [] \n",
    "    total_gears_number = []\n",
    "    number_false_positives_per_image = []\n",
    "    # Loop over model score thresholds and calculate precision, recall    \n",
    "    for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
    "        # On first iteration, define img_results for the first time:\n",
    "        img_ids = gt_boxes.keys() if ithr == 0 else model_scores_map[model_score_thr]\n",
    "        for img_id in img_ids:\n",
    "            gt_boxes_img = gt_boxes[img_id]\n",
    "            box_scores = pred_boxes_pruned[img_id]['scores']\n",
    "            start_idx = 0\n",
    "            for score in box_scores:\n",
    "                if score <= model_score_thr:\n",
    "                    pred_boxes_pruned[img_id]\n",
    "                    start_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Remove boxes, scores of lower than threshold scores:\n",
    "            pred_boxes_pruned[img_id]['scores'] = pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
    "            pred_boxes_pruned[img_id]['boxes'] = pred_boxes_pruned[img_id]['boxes'][start_idx:]\n",
    "            # Recalculate image results for this image\n",
    "            img_results[img_id] = get_single_image_results(\n",
    "                gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr)\n",
    "\n",
    "            total_gears_number.append(img_id.split('_')[0]) \n",
    "\n",
    "            if img_results[img_id]['false_pos'] > 0:\n",
    "                false_positives_images_names.append(img_id.split('_')[0])\n",
    "                number_false_positives_per_image.append(img_results[img_id]['false_pos'])\n",
    "\n",
    "        predictions_numbers_dict= pd.DataFrame({\n",
    "        'gear_number':false_positives_images_names,\n",
    "        \"predictions_number\": number_false_positives_per_image,\n",
    "    \n",
    "        })\n",
    "        predictions_numbers_dict= predictions_numbers_dict.set_index('gear_number')   \n",
    "        prec, rec,total_tp,total_fp,total_fn = calc_precision_recall(img_results)\n",
    "        prec = prec * 100\n",
    "        prec = np.round(prec)\n",
    "        rec = rec * 100\n",
    "        rec = np.round(rec)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        model_thrs.append(model_score_thr)\n",
    "     \n",
    "        #break \n",
    "\n",
    "\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    \n",
    "\n",
    "\n",
    "    prec_at_rec = []\n",
    "    for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "        try:\n",
    "            args = np.argwhere(recalls >= recall_level).flatten()\n",
    "            prec = max(precisions[args])\n",
    "        except ValueError:\n",
    "            prec = 0.0\n",
    "        prec_at_rec.append(prec)\n",
    "\n",
    "    avg_prec = np.mean(prec_at_rec)\n",
    "    return { 'Threshold':model_score_thr,\n",
    "             'True_positives': total_tp,\n",
    "             'False_positives': total_fp,\n",
    "             'False_negatives': total_fn,\n",
    "             'Precision': prec,\n",
    "             'Recall': rec,\n",
    "             'avg_prec': avg_prec,\n",
    "             'precisions': precisions,\n",
    "             'recalls': recalls,\n",
    "             'model_thrs': model_thrs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection of the test set images with different thresholds\n",
    "different_thresholds = [.75,.8,.85,.9,.95]\n",
    "print('catalog_test images number',len(catalog_test)) # number of test images\n",
    "precisions_for_chart = []\n",
    "recalls_for_chart = []\n",
    "for threshold_for_json in different_thresholds:\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold_for_json  # Sets threshold for this model\n",
    "    predictor = detectron2.engine.DefaultPredictor(cfg)\n",
    "    report_metrics(threshold_for_json)      \n",
    "    with open('ground_truth_chipped_all'+str(threshold_for_json)+'.json') as infile:\n",
    "        gt_boxes = json.load(infile)   \n",
    "    with open('predictions_chipped_all'+str(threshold_for_json)+'.json') as infile:\n",
    "        pred_boxes = json.load(infile)\n",
    "    iou_thr = 0.5    \n",
    "    data = get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=iou_thr)\n",
    "    precisions_for_chart.append(data['Precision'])\n",
    "    recalls_for_chart.append(data['Recall'])\n",
    "    print('Metrics for threshold '+str(threshold_for_json)+' are', data)\n",
    "print(precisions_for_chart)\n",
    "print(recalls_for_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting bar chart for precision and recall with different thresholds\n",
    "N = 5 \n",
    "precision = precisions_for_chart\n",
    "recall = recalls_for_chart\n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(N) \n",
    "width = 0.15       \n",
    "precision_bar = ax.bar(ind, precision, width, label='Precision')\n",
    "recall_bar = ax.bar(ind + width, recall, width,\n",
    "    label='Recall')\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom',fontsize=20)\n",
    "autolabel(precision_bar)\n",
    "autolabel(recall_bar)       \n",
    "\n",
    "# for i, v in enumerate(precision):\n",
    "#     ax.text(v + 3, i + .25, str(v), color='blue', fontweight='bold')\n",
    "    \n",
    "ax.set_xlabel('Threshold',fontsize=20)\n",
    "ax.set_ylabel('Percentage',fontsize=20)\n",
    "ax.set_title('Precision and recall of 300 images',fontsize=20)\n",
    "\n",
    "\n",
    "plt.xticks(ind + width / 2, ('75%', '80%', '85%', '90%', '95%'),fontsize=20)\n",
    "plt.legend(loc='upper right',prop={\"size\":10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run these cells to plot the precision and recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0 # set to 0 to plot precision and recall values at all thresholds\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0 # non-maximum suppression \n",
    "\n",
    "cfg.DATASETS.TEST = (\"test\", )\n",
    "#predictor = DefaultPredictor(cfg)\n",
    "\n",
    "predictor = detectron2.engine.DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict ={}\n",
    "ground_truth_dict = {}\n",
    "false_positives_images = []\n",
    "false_negatives_images = []\n",
    "total_ground_truth = 0\n",
    "true_defects_number = 0\n",
    "print('catalog_test images number',len(catalog_test))\n",
    "for example in catalog_test :\n",
    "    \n",
    "    annotations_array=[]\n",
    "    img = cv2.imread(example[\"file_name\"])\n",
    "    prediction = predictor(img)\n",
    "    visualizer_ground_truth = detectron2.utils.visualizer.Visualizer(img[:, :, ::-1], metadata_test, scale=1)  \n",
    "    visualizer_predictions = detectron2.utils.visualizer.Visualizer(img[:, :, ::-1], metadata_test, scale=1)\n",
    "    #show ground truth\n",
    "    vis_ground_truth = visualizer_ground_truth.draw_dataset_dict(example)\n",
    "    #show predictions\n",
    "    vis_predictions = visualizer_predictions.draw_instance_predictions(prediction[\"instances\"].to(\"cpu\"))   \n",
    "    \n",
    "    # Getting bboxes for ground truth\n",
    "    \n",
    "    total_ground_truth = total_ground_truth + len(example['annotations'])\n",
    "    for anno in range (len(example['annotations'])):\n",
    "        bounding_coord = example['annotations'][anno]['bbox']\n",
    "        \n",
    "        modified_boxes = [bounding_coord[0],bounding_coord[1],bounding_coord[0]+bounding_coord[2],bounding_coord[1]+bounding_coord[3]]\n",
    "        annotations_array.append(modified_boxes)\n",
    "    ground_dict= {\n",
    "      os.path.basename(example[\"file_name\"]):annotations_array\n",
    "    }\n",
    "    ground_truth_dict.update(ground_dict) \n",
    "    with open(\"ground_truth.json\", \"w\") as outfile: \n",
    "        json.dump(ground_truth_dict, outfile)\n",
    "    \n",
    "   \n",
    "    predictions_boxes = prediction[\"instances\"].get('pred_boxes')\n",
    "    predictions_scores = prediction[\"instances\"].get('scores')\n",
    "    boxes_array=[]\n",
    "    scores_array=[]\n",
    "    \n",
    "    # Getting bboxes and scores for predicitons\n",
    "    \n",
    "    for predict in range(len(predictions_boxes)):\n",
    "\n",
    "        coordinates = [round(float(list(predictions_boxes)[predict][0]),4),round(float(list(predictions_boxes)[predict][1]),4),round(float(list(predictions_boxes)[predict][2]),4),round(float(list(predictions_boxes)[predict][3]),4)]      \n",
    "        scores = round(float(predictions_scores[predict]),4)     \n",
    "        boxes_array.append(coordinates)\n",
    "        scores_array.append(scores)\n",
    "    pred_dict= {\n",
    "      os.path.basename(example[\"file_name\"]):{\n",
    "      \"boxes\": boxes_array,\n",
    "      \"scores\": scores_array\n",
    "    }}   \n",
    "    predictions_dict.update(pred_dict) \n",
    "    with open(\"predictions.json\", \"w\") as outfile: \n",
    "        json.dump(predictions_dict, outfile) \n",
    "print(\"DONE\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=0.5):\n",
    "    \"\"\"Calculates average precision at given IoU threshold.\n",
    "\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (list of list of floats): list of locations of predicted\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "\n",
    "    Returns:\n",
    "        dict: avg precision as well as summary info about the PR curve\n",
    "\n",
    "        Keys:\n",
    "            'avg_prec' (float): average precision for this IoU threshold\n",
    "            'precisions' (list of floats): precision value for the given\n",
    "                model_threshold\n",
    "            'recall' (list of floats): recall value for given\n",
    "                model_threshold\n",
    "            'models_thrs' (list of floats): model threshold value that\n",
    "                precision and recall were computed for.\n",
    "    \"\"\"\n",
    "\n",
    "    model_scores_map = get_model_scores_map(pred_boxes)\n",
    "    sorted_model_scores = sorted(model_scores_map.keys())\n",
    "    \n",
    "\n",
    "    # Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
    "    for img_id in pred_boxes.keys():\n",
    "        arg_sort = np.argsort(pred_boxes[img_id]['scores'])\n",
    "        pred_boxes[img_id]['scores'] = np.array(pred_boxes[img_id]['scores'])[arg_sort].tolist()\n",
    "        pred_boxes[img_id]['boxes'] = np.array(pred_boxes[img_id]['boxes'])[arg_sort].tolist()\n",
    "\n",
    "    pred_boxes_pruned = deepcopy(pred_boxes)\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    model_thrs = []\n",
    "    img_results = {}\n",
    "    false_positives_images_names = [] \n",
    "    total_gears_number = []\n",
    "    number_false_positives_per_image = []\n",
    "    # Loop over model score thresholds and calculate precision, recall\n",
    "    \n",
    "   \n",
    "    for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
    "        # On first iteration, define img_results for the first time:\n",
    "        img_ids = gt_boxes.keys() if ithr == 0 else model_scores_map[model_score_thr]\n",
    "        for img_id in img_ids:\n",
    "            gt_boxes_img = gt_boxes[img_id]\n",
    "            box_scores = pred_boxes_pruned[img_id]['scores']\n",
    "            start_idx = 0\n",
    "            for score in box_scores:\n",
    "                if score <= model_score_thr:\n",
    "                    pred_boxes_pruned[img_id]\n",
    "                    start_idx += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Remove boxes, scores of lower than threshold scores:\n",
    "            pred_boxes_pruned[img_id]['scores'] = pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
    "            pred_boxes_pruned[img_id]['boxes'] = pred_boxes_pruned[img_id]['boxes'][start_idx:]\n",
    "\n",
    "            \n",
    "            img_results[img_id] = get_single_image_results(\n",
    "                gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr)\n",
    "\n",
    "            total_gears_number.append(img_id.split('_')[0]) # I added this to get the total number of gears\n",
    "\n",
    "            if img_results[img_id]['false_pos'] > 0:\n",
    "                false_positives_images_names.append(img_id.split('_')[0])\n",
    "                number_false_positives_per_image.append(img_results[img_id]['false_pos'])\n",
    "\n",
    "       \n",
    "        predictions_numbers_dict= pd.DataFrame({\n",
    "        'gear_number':false_positives_images_names,\n",
    "        \"predictions_number\": number_false_positives_per_image,\n",
    "    \n",
    "        })\n",
    "        predictions_numbers_dict= predictions_numbers_dict.set_index('gear_number')\n",
    "\n",
    "        prec, rec,total_tp,total_fp,total_fn = calc_precision_recall(img_results)\n",
    "\n",
    "\n",
    "\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "\n",
    "        model_thrs.append(model_score_thr)\n",
    "\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "\n",
    "    prec_at_rec = []\n",
    "    for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "        try:\n",
    "            args = np.argwhere(recalls >= recall_level).flatten()\n",
    "            prec = max(precisions[args])\n",
    "        except ValueError:\n",
    "            prec = 0.0\n",
    "        prec_at_rec.append(prec)\n",
    "\n",
    "    avg_prec = np.mean(prec_at_rec)\n",
    "\n",
    "    return { 'Threshold':model_score_thr,\n",
    "             'True_positives': total_tp,\n",
    "             'False_positives': total_fp,\n",
    "             'False_negatives': total_fn,\n",
    "             'Precision': prec,\n",
    "             'Recall': rec,\n",
    "             'avg_prec': avg_prec,\n",
    "             'precisions': precisions,\n",
    "             'recalls': recalls,\n",
    "             'model_thrs': model_thrs}\n",
    "\n",
    "\n",
    "def plot_pr_curve(\n",
    "    precisions, recalls, category='Chipped-Tooth', label=None, color=None, ax=None):\n",
    "    \"\"\"Simple plotting helper function\"\"\"\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10,8))\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if color is None:\n",
    "        color = COLORS[0]\n",
    "    ax.scatter(recalls, precisions, label=label, s=5, color='#1f77b4') \n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "\n",
    "    ax.set_title('Precision-Recall curve of Chipped-Tooth Defect (306 images)')\n",
    "    #ax.set_title('Precision-Recall curve of damaged teeth 10th fold')\n",
    "    ax.set_xlim([0.0,1.1])\n",
    "    ax.set_ylim([0.0,1.1])\n",
    "    return ax\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with open('ground_truth.json') as infile:\n",
    "        gt_boxes = json.load(infile)\n",
    "\n",
    "    with open('predictions.json') as infile:\n",
    "        pred_boxes = json.load(infile)\n",
    "\n",
    "    # Runs it for one IoU threshold\n",
    "    iou_thr = 0.5 \n",
    "   \n",
    "    data = get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=iou_thr)\n",
    "    ax = None\n",
    "    avg_precs = []\n",
    "    iou_thrs = []\n",
    "    recalls_printed = []\n",
    "\n",
    "   \n",
    "    avg_precs.append(data['avg_prec'])\n",
    "    iou_thrs.append(iou_thr)\n",
    "\n",
    "\n",
    "    precisions = data['precisions']\n",
    "    recalls = data['recalls']\n",
    "\n",
    "    \n",
    "    for idx, iou_thr in enumerate(np.linspace(0.5, 0.5,10)):\n",
    "\n",
    "        data = get_avg_precision_at_iou(gt_boxes, pred_boxes, iou_thr=iou_thr)\n",
    "        avg_precs.append(data['avg_prec'])\n",
    "        iou_thrs.append(iou_thr)\n",
    "        precisions = data['precisions']\n",
    "        recalls = data['recalls']\n",
    "        ax = plot_pr_curve(\n",
    "            precisions, recalls, label='{:.2f}'.format(iou_thr), color=COLORS[idx*2], ax=ax)\n",
    "\n",
    "    # prettify for printing:\n",
    "    avg_precs = [float('{:.4f}'.format(ap)) for ap in avg_precs]\n",
    "    iou_thrs = [float('{:.4f}'.format(thr)) for thr in iou_thrs]\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    for xval in np.linspace(0.0, 1.0, 11):\n",
    "        plt.vlines(xval, 0.0, 1.1, color='gray', alpha=0.3, linestyles='dashed')\n",
    "        plt.hlines(xval, 0.0, 1.1, color='gray', alpha=0.3, linestyles='dashed')\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
